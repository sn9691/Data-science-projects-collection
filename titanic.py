# -*- coding: utf-8 -*-
"""titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xF0mLNtlCRC8t8iGwSi-E3pKs0uHyXIX
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
plt.rcParams['figure.figsize'] = (15,10)

"""# Input"""

# from google.colab import files
# train = files.upload()
# test = files.upload()

# import io
# train = pd.read_csv(io.BytesIO(train['train.csv']))
# test = pd.read_csv(io.BytesIO(test['test.csv']))

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

titanic = train.append(test, ignore_index=True)
train_idx = len(train)
test_idx = len(titanic) - len(test)

titanic.info()

titanic['Embarked'].fillna(titanic['Embarked'].mode()[0], inplace=True)

"""## notna: titanic data without nulls, used for training
## isna: titanic data with nulls, used for target
"""

notna = titanic[titanic['Cabin'].notna()]

notna.info()

notna.drop(['Name', 'Ticket', 'Age'], inplace=True, axis=1)

deck = []

for x in notna['Cabin']:
    y = x.split(' ')
    deck.append(y[0][0:1])

notna['Deck'] = deck

isna = titanic[titanic['Cabin'].isna()]

isna.drop(['Age', 'Name', 'Ticket', 'Cabin', 'Survived'], axis=1, inplace=True)
notna.drop(['Cabin', 'Survived'], axis=1, inplace=True)

isna['Fare'].fillna(round(isna['Fare'].median()), inplace=True)

isna.info()

notna.info()

"""# Predicting the 'Deck' column

### training data -> notna
### target data -> isna
"""

X = pd.get_dummies(notna.iloc[:, :-1])
y = notna['Deck']
target = pd.get_dummies(isna)

from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier()

tree.fit(X,y)

pred = []

pred = pred + list(tree.predict(target[0:295]))

pred = pred + list(tree.predict(target[295:590]))

pred = pred + list(tree.predict(target[590:885]))

pred = pred + list(tree.predict(target[885:]))

isna['Deck'] = pred

isna.info()

notna.info()

"""### Fitting the 'Deck' column"""

temp = [None]*1309
titanic['Deck'] = temp

passid_1 = list(notna['PassengerId'])

for x in list(titanic[titanic['PassengerId'].isin(passid_1)].index):
    titanic['Deck'][x] = notna['Deck'][x]

passid_2 = list(isna['PassengerId'])

for x in list(titanic[titanic['PassengerId'].isin(passid_2)].index):
    titanic['Deck'][x] = isna['Deck'][x]

titanic.drop('Cabin',inplace=True, axis=1)

titanic.info()

titanic['Fare'].fillna(titanic['Fare'].median(), inplace=True)

titanic['FamilySize'] = titanic['SibSp'] + titanic['Parch']

titanic.drop(['SibSp', 'Parch'], axis=1, inplace=True)

"""# Predicting the 'Age' column"""

titanic.groupby(['Sex', 'Pclass']).median()['Age']

indices = list(titanic[titanic['Age'].isna()].index)

len(indices)

indices_m = []
indices_f = []

for x in indices:
  if titanic['Sex'][x] == 'male':
    indices_m.append(x)
  else:
    indices_f.append(x)

for x in indices_m:
  if titanic['Pclass'][x] == 1:
    titanic['Age'][x] = 36.0
  elif titanic['Pclass'][x] == 2:
    titanic['Age'][x] = 28.0
  else:
    titanic['Age'][x] = 22.0

for x in indices_f:
  if titanic['Pclass'][x] == 1:
    titanic['Age'][x] = 42.0
  elif titanic['Pclass'][x] == 2:
    titanic['Age'][x] = 29.0
  else:
    titanic['Age'][x] = 25.0

titanic.info()

titanic.drop(['Ticket', 'Name'], axis=1, inplace=True)

"""# --------------------------------------------------------------------

# !!! Stop here and review code for more feature enineering !!!

# ---------------------------------------------------------------------

# Caution! Conversion of data into dummies and arrays below
"""

dummies = pd.get_dummies(titanic)

dummies.info()

train = dummies[:train_idx]
test = dummies[test_idx:]

train['Survived'] = train['Survived'].astype(int)

test.drop('Survived', axis=1, inplace=True)

X = train.drop('Survived', axis=1).values
y = train['Survived'].values

"""# ----------------------------------

# Machine Learning starts here

# ----------------------------------

# Using Random Forest
"""

param_dict = {
    'max_depth': [n for n in range(9, 14)],     
    'min_samples_split': [n for n in range(4, 11)], 
    'min_samples_leaf': [n for n in range(2, 5)],     
    'n_estimators': [n for n in range(10, 60, 10)] }

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold

skf = StratifiedKFold(n_splits=5, random_state=None)
grid = GridSearchCV(estimator=forest, param_grid=param_dict, cv=5)

'''grid search on random forest'''

grid.fit(X,y)
grid.best_params_

forest = RandomForestClassifier(max_depth=12, min_samples_leaf=2, min_samples_split=8, n_estimators=20)

'''random forest'''
'''without using stratified k fold cross val'''

forest.fit(X_train, y_train)
pred = forest.predict(test.values)
kaggle = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': pred})
kaggle.to_csv('submission_1.csv', index=False)

'''random forest'''
'''using stratified k fold cross val'''

for train_index, test_index in skf.split(X,y): 
    print("Train:", train_index, "Test:", test_index) 
    X_train, X_test = X[train_index], X[test_index] 
    y_train, y_test = y[train_index], y[test_index]
    
    forest.fit(X_train, y_train)
    pred = forest.predict(test.values)

kaggle = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': pred})
kaggle.to_csv('submission_2.csv', index=False)



